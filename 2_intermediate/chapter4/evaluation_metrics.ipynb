{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4: Evaluation Metrics in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfIaH+4xPSEwMK8sRapZgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/code4tomorrow/machine-learning/blob/main/2_intermediate/chapter4/evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_FKXUvtibyQ"
      },
      "source": [
        "### **Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzUvca88h5GV"
      },
      "source": [
        "Welcome to Evalution Metrics in Python! In this notebook, we will be briefly going over the code for each evaluation metric, so that you know how to apply it when you implement further Machine Learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ufzm6sKi-D8"
      },
      "source": [
        "First, we will import the metrics library from Sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYY1W6TPiiKR"
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbe-sdXnjI7_"
      },
      "source": [
        "### **Accuracy Score (Similar to Jaccard Score)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHXdAd4jDG-",
        "outputId": "b2decb9d-80a4-4eb7-b21f-740cf3ffaee3"
      },
      "source": [
        "y_test = [1, 2] #This initialized list represents the test set output\n",
        "y_predict = [1, 3] #This initialized list represents the prediction from the model\n",
        "print(metrics.accuracy_score(y_test, y_predict)) #This is the syntax to calculate the accuracy score between y_test and y_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtN6QvrcmN2A"
      },
      "source": [
        "### **Jaccard Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUXueSummQQ2",
        "outputId": "ebec9c93-65d4-4e1c-c073-c375579a8498"
      },
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "y_test = [1, 2] #This initialized list represents the test set output\n",
        "y_predict = [1, 3] #This initialized list represents the prediction from the model\n",
        "print(jaccard_score(y_test,y_predict, average = \"weighted\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m542MoLHkO7_"
      },
      "source": [
        "### **Log Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj9wvrmqi7Vz",
        "outputId": "5dfc1f95-5f09-49c9-bd4d-fc479d3a3b39"
      },
      "source": [
        "from sklearn.metrics import log_loss #We have to import the Log Loss Score seperately\n",
        "y_test = [1, 0] #This list also represents test set information\n",
        "y_predict = [0.9,0.3] #This represents the predictions. The difference between this y_predict and the above one is that Log Loss takes probabilities, not classes, so we had to change it to probabilities.\n",
        "print(log_loss(y_test, y_predict)) #Prints the Log Loss score for these two lists."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17833747196936672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYqiYZQglTA8"
      },
      "source": [
        "### **F1 Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrGPNMyMk0TM",
        "outputId": "861d6077-2eb7-4d83-f7f0-9c397aec9be8"
      },
      "source": [
        "from sklearn.metrics import f1_score #This is how we import the f1_score\n",
        "y_test = [1,2]\n",
        "y_predict = [1,3] #The F1-Score uses class predictions\n",
        "print(f1_score(y_test,y_predict, average = \"weighted\")) #We can just use the weighted average as the standard setting."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ju4pCxwmzI5"
      },
      "source": [
        "And that's all for this Notebook! Thanks for going through this."
      ]
    }
  ]
}